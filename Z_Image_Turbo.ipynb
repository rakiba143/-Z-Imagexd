{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5NUBaPey9U2G"
      },
      "outputs": [],
      "source": [
        "#@title Install Z-Image-Turbo\n",
        "#ComfyUI Installation\n",
        "%cd /content/\n",
        "!git clone https://github.com/comfyanonymous/ComfyUI\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "PIN_COMMIT = \"3c8456223c5f6a41af7d99219b391c8c58acb552\"  #fixed version\n",
        "# PIN_COMMIT=\"\" #latest version\n",
        "if PIN_COMMIT:\n",
        "    !git fetch --all -q\n",
        "    !git reset --hard {PIN_COMMIT}\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/NeuralFalconYT/Z-Image-Colab/refs/heads/main/app.py\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "#Model Download\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/T5B/Z-Image-Turbo-FP8/resolve/main/z-image-turbo-fp8-e4m3fn.safetensors -d /content/ComfyUI/models/diffusion_models -o z-image-turbo-fp8-e4m3fn.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/text_encoders/qwen_3_4b.safetensors -d /content/ComfyUI/models/clip -o qwen_3_4b.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/vae/ae.safetensors -d /content/ComfyUI/models/vae -o ae.safetensors\n",
        "\n",
        "#Clear Terminal\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"\\033[92mZ-Image-Turbo Installation successful\\033[0m\")\n",
        "import os\n",
        "\n",
        "paths = [\n",
        "    \"/content/ComfyUI/models/diffusion_models/z-image-turbo-fp8-e4m3fn.safetensors\",\n",
        "    \"/content/ComfyUI/models/clip/qwen_3_4b.safetensors\",\n",
        "    \"/content/ComfyUI/models/vae/ae.safetensors\",\n",
        "]\n",
        "\n",
        "for p in paths:\n",
        "    if not os.path.exists(p):\n",
        "        print(f\"\\033[91mMISSING (possibly HuggingFace blocked z-image-turbo model download, Run the Colab Cell again): {p}\\033[0m\")\n",
        "\n",
        "#@title Keep Alive for Mobile Users\n",
        "from IPython.display import Audio,display\n",
        "display(Audio(\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\", autoplay=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run from colab cell"
      ],
      "metadata": {
        "id": "J9Xllo8CmWuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utils Code\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "import os, random, time\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import re, uuid\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "CLIPLoader = NODE_CLASS_MAPPINGS[\"CLIPLoader\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
        "KSampler = NODE_CLASS_MAPPINGS[\"KSampler\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    unet = UNETLoader.load_unet(\"z-image-turbo-fp8-e4m3fn.safetensors\", \"fp8_e4m3fn_fast\")[0]\n",
        "    clip = CLIPLoader.load_clip(\"qwen_3_4b.safetensors\", type=\"lumina2\")[0]\n",
        "    vae = VAELoader.load_vae(\"ae.safetensors\")[0]\n",
        "\n",
        "save_dir=\"./results\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "def get_save_path(prompt):\n",
        "  save_dir = \"./results\"\n",
        "  safe_prompt = re.sub(r'[^a-zA-Z0-9_-]', '_', prompt)[:25]\n",
        "  uid = uuid.uuid4().hex[:6]\n",
        "  filename = f\"{safe_prompt}_{uid}.png\"\n",
        "  path = os.path.join(save_dir, filename)\n",
        "  return path\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(input):\n",
        "    values = input[\"input\"]\n",
        "    positive_prompt = values['positive_prompt']\n",
        "    negative_prompt = values['negative_prompt']\n",
        "    seed = values['seed'] # 0\n",
        "    steps = values['steps'] # 9\n",
        "    cfg = values['cfg'] # 1.0\n",
        "    sampler_name = values['sampler_name'] # euler\n",
        "    scheduler = values['scheduler'] # simple\n",
        "    denoise = values['denoise'] # 1.0\n",
        "    width = values['width'] # 1024\n",
        "    height = values['height'] # 1024\n",
        "    batch_size = values['batch_size'] # 1.0\n",
        "\n",
        "    if seed == 0:\n",
        "        random.seed(int(time.time()))\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "\n",
        "    positive = CLIPTextEncode.encode(clip, positive_prompt)[0]\n",
        "    negative = CLIPTextEncode.encode(clip, negative_prompt)[0]\n",
        "    latent_image = EmptyLatentImage.generate(width, height, batch_size=batch_size)[0]\n",
        "    samples = KSampler.sample(unet, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)[0]\n",
        "    decoded = VAEDecode.decode(vae, samples)[0].detach()\n",
        "    save_path=get_save_path(positive_prompt)\n",
        "    Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(save_path)\n",
        "    return save_path,seed"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GpDjqdueK1Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# üî• AUTO START API (Run All & Relax)\n",
        "# ==========================================\n",
        "import os, time, sys, threading, subprocess\n",
        "\n",
        "# ‡ßß. ‡¶Ö‡¶ü‡ßã ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™\n",
        "try:\n",
        "    import uvicorn, nest_asyncio, pyngrok\n",
        "    from fastapi import FastAPI, Response\n",
        "    from pydantic import BaseModel\n",
        "except:\n",
        "    os.system(\"pip install -q fastapi uvicorn pyngrok nest-asyncio\")\n",
        "    import uvicorn, nest_asyncio, pyngrok\n",
        "    from fastapi import FastAPI, Response\n",
        "    from pydantic import BaseModel\n",
        "\n",
        "os.system(\"fuser -k 8000/tcp\")\n",
        "\n",
        "# ‡ß®. ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™ ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶®\n",
        "app = FastAPI()\n",
        "queue_lock = threading.Lock()\n",
        "\n",
        "class UserRequest(BaseModel):\n",
        "    positive_prompt: str\n",
        "    negative_prompt: str = \"low quality, bad anatomy, blurry\"\n",
        "    width: int = 720\n",
        "    height: int = 1280\n",
        "    steps: int = 10\n",
        "    cfg: float = 1.0\n",
        "    seed: int = 0\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "def api_generate(req: UserRequest):\n",
        "    with queue_lock:\n",
        "        input_data = {\n",
        "          \"input\": {\n",
        "            \"positive_prompt\": req.positive_prompt,\n",
        "            \"negative_prompt\": req.negative_prompt,\n",
        "            \"width\": req.width, \"height\": req.height,\n",
        "            \"batch_size\": 1, \"seed\": req.seed, \"steps\": req.steps,\n",
        "            \"cfg\": req.cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1.0,\n",
        "          }\n",
        "        }\n",
        "        try:\n",
        "            if 'generate' not in globals(): return Response(content=\"Model Loading... Please wait.\", status_code=500)\n",
        "            image_path, seed = generate(input_data)\n",
        "            with open(image_path, \"rb\") as f: img_bytes = f.read()\n",
        "            return Response(content=img_bytes, media_type=\"image/png\")\n",
        "        except Exception as e: return Response(content=str(e), status_code=500)\n",
        "\n",
        "def keep_alive():\n",
        "    while True: time.sleep(60); print(\".\", end=\"\", flush=True)\n",
        "threading.Thread(target=keep_alive, daemon=True).start()\n",
        "\n",
        "# ‡ß©. ‡¶ü‡ßã‡¶ï‡ßá‡¶® ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™ (‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ü‡ßã‡¶ï‡ßá‡¶® ‡¶¶‡¶ø‡¶®)\n",
        "ngrok_token = \"39MLZulT6VaVF91NJwfCkdTJlkl_7yQUmuyN8bWxPHXYVkU2r\"\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "try:\n",
        "    public_url = ngrok.connect(8000).public_url\n",
        "    print(f\"\\nüöÄ API LINK: {public_url}/generate\\n\")\n",
        "except: pass\n",
        "\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "id": "ZBpeMajQiSnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}